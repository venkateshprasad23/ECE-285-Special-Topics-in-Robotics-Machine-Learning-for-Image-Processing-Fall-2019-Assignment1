{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Venkatesh Prasad Venkataramanan A53318036 Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load in module MNISTtools:\n",
      "\n",
      "load(dataset='training', path=None)\n",
      "    Import either the training or testing MNIST data set.\n",
      "    It returns a pair with the first element being the collection of\n",
      "    images stacked in columns and the second element being a vector\n",
      "    of corresponding labels from 0 to 9.\n",
      "    \n",
      "    Arguments:\n",
      "        dataset (string, optional): either \"training\" or \"testing\".\n",
      "            (default: \"training\")\n",
      "        path (string, optional): the path pointing to the MNIST dataset\n",
      "            If path=None, it looks succesively for the dataset at:\n",
      "            '/datasets/MNIST' and './MNIST'. (default: None)\n",
      "    \n",
      "    Example:\n",
      "        x, lbl = load(dataset=\"testing\", path=\"/Folder/for/MNIST\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MNISTtools.load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1\n",
      "The shape of xtrain is : (784, 60000)\n",
      "The shape of ltrain is : (60000,)\n",
      "The size of the training dataset is :  60000\n",
      "The feature dimension is :  784\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 1\")\n",
    "\n",
    "xtrain, ltrain = MNISTtools.load(dataset=\"training\", path=\"/datasets/MNIST\")#loaded data\n",
    "\n",
    "print (\"The shape of xtrain is :\",xtrain.shape)\n",
    "print (\"The shape of ltrain is :\",ltrain.shape)\n",
    "print (\"The size of the training dataset is : \",xtrain.shape[1])\n",
    "print (\"The feature dimension is : \",xtrain.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltrain value for that index is : 7\n",
      "They are similar\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 2\")\n",
    "\n",
    "MNISTtools.show(xtrain[:, 42])#displaying the image at that index\n",
    "\n",
    "print (\"ltrain value for that index is :\",ltrain[42])\n",
    "print (\"They are similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 3\n",
      "The minimum of xtrain is : 0\n",
      "The maximum of xtrain is : 255\n",
      "The type of xtrain is : uint8\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 3\")\n",
    "\n",
    "print(\"The minimum of xtrain is :\",np.amin(xtrain))\n",
    "print(\"The maximum of xtrain is :\",np.amax(xtrain))\n",
    "print(\"The type of xtrain is :\",xtrain.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 4\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 4\")\n",
    "\n",
    "xtrain = xtrain.astype(np.float32)#float conversion\n",
    "\n",
    "def normalize_MNIST_images(x):\n",
    "    x = -1.0 + (2*x)/255#mapping [0,255] to [-1,1]\n",
    "    return x\n",
    "\n",
    "\n",
    "xtrain = normalize_MNIST_images(xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 5\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "7\n",
      "We can see that the one-hot code matches.\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 5\")\n",
    "\n",
    "def label2onehot(lbl):#function to convert labels to one-hot codes\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    d[lbl, np.arange(lbl.size)] = 1\n",
    "    return d\n",
    "\n",
    "dtrain = label2onehot(ltrain)\n",
    "print (dtrain[:,42])\n",
    "print (ltrain[42])\n",
    "print (\"We can see that the one-hot code matches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 6\n",
      "Converting back from one-hot to label : 7\n",
      "We can see that it matches ltrain[42]\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 6\")\n",
    "\n",
    "def onehot2label(d):#function to convert one-hot codes back to label\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl\n",
    "\n",
    "ltrain = onehot2label(dtrain)\n",
    "print(\"Converting back from one-hot to label :\",ltrain[42])\n",
    "print (\"We can see that it matches ltrain[42]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 7\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 7\")\n",
    "\n",
    "def softmax(a):#softmax function\n",
    "    M = np.amax(a)\n",
    "    num = den = np.exp(a - M)\n",
    "    y= ((num)/(den.sum(axis=0)))\n",
    "    return y   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 10\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 10\")\n",
    "\n",
    "def softmaxp(a,e):\n",
    "    y =  softmax(a)\n",
    "    element_wise = np.multiply(y,e)\n",
    "    ans = ((element_wise) - (element_wise.sum(axis=0)*y))\n",
    "    return ans\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 11\n",
      "5.051861861607455e-07 should be smaller than 1e-6\n",
      "As we can see, it is smaller than the said value\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 11\")\n",
    "\n",
    "eps         = 1e-6                        # finite difference step\n",
    "a           = np.random.randn(10, 200)    # random inputs\n",
    "e           = np.random.randn(10, 200)    # random directions\n",
    "diff        = softmaxp(a, e)\n",
    "diff_approx = (softmax(a + eps*e) - softmax(a))/(eps)#the approximation formula\n",
    "rel_error   = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "\n",
    "print(rel_error, 'should be smaller than 1e-6')\n",
    "print(\"As we can see, it is smaller than the said value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 12\n",
      "4.147808391169573e-11 should be smaller than 1e-6\n",
      "As we can see, it is smaller than the said value\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 12\")\n",
    "\n",
    "def relu(a):\n",
    "    return np.maximum(a, 0)\n",
    "\n",
    "def relup(a,e):\n",
    "    y = (a > 0) * e\n",
    "    return y\n",
    "\n",
    "eps         = 1e-6                        # finite difference step\n",
    "a           = np.random.randn(10, 200)    # random inputs\n",
    "e           = np.random.randn(10, 200)    # random directions\n",
    "diff        = relup(a, e)\n",
    "diff_approx = (relu(a + eps*e) - relu(a))/(eps)\n",
    "rel_error   = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "\n",
    "print(rel_error, 'should be smaller than 1e-6')\n",
    "print (\"As we can see, it is smaller than the said value\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 13\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 13\")\n",
    "\n",
    "def init_shallow(Ni, Nh, No):#initiate a network of random values\n",
    "    b1 = np.random.randn(Nh, 1)   / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni)  / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1)   / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh)  / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 14\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 14\")\n",
    "\n",
    "def forwardprop_shallow(x, net):#forward propagate over the network\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "\n",
    "    a1 = W1.dot(x) + b1\n",
    "    #COMPLETE\n",
    "    h1      = relu(a1)\n",
    "    a2      = W2.dot(h1) + b2\n",
    "    y       = softmax(a2)\n",
    "\n",
    "    return y\n",
    "\n",
    "yinit = forwardprop_shallow(xtrain, netinit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 15\n",
      "0.2798442630796185 should be around .26\n",
      "Hence we can see that it satisfies the criteria\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 15\")\n",
    "\n",
    "def eval_loss(y, d):#evaluates average cross-entropy loss\n",
    "    loss_vector = np.multiply(d,np.log(y))\n",
    "    loss = - np.mean(loss_vector)\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "print(eval_loss(yinit, dtrain), 'should be around .26')\n",
    "print(\"Hence we can see that it satisfies the criteria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 16\n",
      "[False False False ... False False False]\n",
      "The percentage of misclassified samples is :  84.035\n",
      "This is because since a random network is used for classification, there is 1/10 probability that the prediction is correct. Hence we are getting 90% misclassified samples.\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 16\")\n",
    "\n",
    "def eval_perfs(y, lbl):#calculates percentage of misclassified samples\n",
    "    c = np.equal(onehot2label(y), lbl)\n",
    "    print (c)\n",
    "    #c = np.sum(c)\n",
    "    c = np.mean(c)*100\n",
    "    return 100 - c\n",
    "    \n",
    "print(\"The percentage of misclassified samples is : \",eval_perfs(yinit, ltrain))\n",
    "print(\"This is because since a random network is used for classification, there is 1/10 probability that the prediction is correct. Hence we are getting 90% misclassified samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 17\n",
      "The proof has been attached.\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 17\")\n",
    "\n",
    "def update_shallow(x, d, net, gamma=.05):#updating weights and biases\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "    # \n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    # \n",
    "    d2 = softmaxp(a2, -d/y)\n",
    "    d1 = relup(a1, W2.T.dot(d2))\n",
    "    # \n",
    "    W2 -= gamma*d2.dot(h1.T)\n",
    "    W1 -= gamma*d1.dot(x.T)\n",
    "    b2 -= gamma*d2.sum(axis=1).reshape(No,1)\n",
    "    b1 -= gamma*d1.sum(axis=1).reshape(Nh,1)\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "print (\"The proof has been attached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 18\n",
      "[False False False ... False False False]\n",
      "The loss is : 0.24643066862020943\n",
      "The training error is : 86.56166666666667\n",
      "[False  True  True ... False False False]\n",
      "The loss is : 0.21458497587880035\n",
      "The training error is : 77.66499999999999\n",
      "[False  True False ... False False False]\n",
      "The loss is : 0.20302100363945652\n",
      "The training error is : 69.55\n",
      "[False  True  True ... False False False]\n",
      "The loss is : 0.19459569098275897\n",
      "The training error is : 61.608333333333334\n",
      "[False  True False ... False False False]\n",
      "The loss is : 0.18663561633810807\n",
      "The training error is : 58.501666666666665\n",
      "[False  True  True ... False False False]\n",
      "The loss is : 0.17892936321212605\n",
      "The training error is : 52.71333333333333\n",
      "[False  True False ...  True False False]\n",
      "The loss is : 0.17165282837925724\n",
      "The training error is : 51.31\n",
      "[False  True  True ... False False False]\n",
      "The loss is : 0.1648618877425437\n",
      "The training error is : 46.39666666666666\n",
      "[False  True False ...  True False False]\n",
      "The loss is : 0.15869486872567937\n",
      "The training error is : 46.599999999999994\n",
      "[False  True  True ... False False False]\n",
      "The loss is : 0.15345459127039238\n",
      "The training error is : 41.971666666666664\n",
      "[False  True  True ...  True False False]\n",
      "The loss is : 0.14876623813868733\n",
      "The training error is : 44.145\n",
      "[False  True  True ... False False False]\n",
      "The loss is : 0.14611277842502585\n",
      "The training error is : 40.321666666666665\n",
      "[False  True  True ...  True False False]\n",
      "The loss is : 0.1426181790772697\n",
      "The training error is : 44.17833333333333\n",
      "[False  True  True ... False  True False]\n",
      "The loss is : 0.14328637518293533\n",
      "The training error is : 42.525\n",
      "[False  True  True ...  True False False]\n",
      "The loss is : 0.13869320528593423\n",
      "The training error is : 44.44833333333334\n",
      "[False  True  True ...  True  True False]\n",
      "The loss is : 0.13826042841078734\n",
      "The training error is : 43.06833333333333\n",
      "[False  True  True ...  True False False]\n",
      "The loss is : 0.13393285706232413\n",
      "The training error is : 43.385\n",
      "[False  True  True ...  True  True False]\n",
      "The loss is : 0.12793381563438305\n",
      "The training error is : 38.080000000000005\n",
      "[False  True  True ...  True False False]\n",
      "The loss is : 0.12408399742427799\n",
      "The training error is : 39.651666666666664\n",
      "[False  True  True ...  True  True False]\n",
      "The loss is : 0.11800781816168868\n",
      "The training error is : 33.965\n",
      "[False  True  True ...  True False False]\n",
      "The loss is : 0.1152778002137304\n",
      "The training error is : 35.81\n",
      "[ True  True  True ...  True  True False]\n",
      "The loss is : 0.110507507990097\n",
      "The training error is : 31.599999999999994\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 18\")\n",
    "\n",
    "def backprop_shallow(x, d, net, T, gamma = 0.05):#backpropagation function\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        net = update_shallow(x,d,net,gamma)\n",
    "        y = forwardprop_shallow(x,net)\n",
    "        loss = eval_loss(y,d)\n",
    "        training_error = eval_perfs(y,lbl)\n",
    "        print (\"The loss is :\",loss)\n",
    "        print (\"The training error is :\",training_error)\n",
    "        \n",
    "    return net\n",
    "\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 2)\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 18 Contd\n",
      "As we can see, the network has achieved training error of approx 31% after 20 iterations\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 18 Contd\")\n",
    "print (\"As we can see, the network has achieved training error of approx 31% after 20 iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 19\n",
      "The shape of xtest is: (784, 10000)\n",
      "The shape of ltest is: (10000,)\n",
      "The size of the testing dataset is : 10000\n",
      "[ True False  True ... False  True  True]\n",
      "The loss is :  0.1568102540631892\n",
      "The testing error is:  42.64\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 19\")\n",
    "\n",
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "xtest = normalize_MNIST_images(xtest)\n",
    "dtest = label2onehot(ltest)\n",
    "\n",
    "print(\"The shape of xtest is:\",xtest.shape)\n",
    "print(\"The shape of ltest is:\",ltest.shape)\n",
    "print(\"The size of the testing dataset is : 10000\")\n",
    "\n",
    "y = forwardprop_shallow(xtest,nettrain)#testing on test data\n",
    "loss = eval_loss(y,dtest)\n",
    "testing_error = eval_perfs(y,ltest)\n",
    "\n",
    "print(\"The loss is : \",loss)\n",
    "print (\"The testing error is: \",testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 20\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 20\")\n",
    "\n",
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):#minibatch gradient descent method\n",
    "    N = x.shape[1]\n",
    "    lbl = onehot2label(d)\n",
    "    NB =int((N+B-1)/B)\n",
    "    \n",
    "    for t in range(T):\n",
    "        for l in range(NB):\n",
    "            idx = np.arange(B*l, min(B*(l+1), N))\n",
    "            net = update_shallow(x[:,idx],d[:,idx],net,gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        loss = eval_loss(y,d)\n",
    "        training_error = eval_perfs(y,lbl)\n",
    "        print(loss)\n",
    "        print(training_error)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 21\n",
      "[ True  True  True ...  True  True  True]\n",
      "0.03125747610318479\n",
      "9.504999999999995\n",
      "[ True  True  True ...  True  True  True]\n",
      "0.0245181396793827\n",
      "7.430000000000007\n",
      "[ True  True  True ...  True  True  True]\n",
      "0.020309724811249538\n",
      "6.126666666666665\n",
      "[ True  True  True ...  True  True  True]\n",
      "0.017740092520085952\n",
      "5.388333333333321\n",
      "[ True  True  True ...  True  True  True]\n",
      "0.015481566728655935\n",
      "4.739999999999995\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.039270899049870224\n",
      "The testing error is : 12.200000000000003\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 21\")\n",
    "\n",
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100)\n",
    "y = forwardprop_shallow(xtest,netminibatch)\n",
    "loss = eval_loss(y,dtest)\n",
    "testing_error = eval_perfs(y,ltest)\n",
    "\n",
    "print(\"The loss is :\",loss)\n",
    "print (\"The testing error is :\", testing_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
