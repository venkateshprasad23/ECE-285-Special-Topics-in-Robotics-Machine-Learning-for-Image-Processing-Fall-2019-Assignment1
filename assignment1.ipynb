{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Venkatesh Prasad Venkataramanan A53318036 Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load in module MNISTtools:\n",
      "\n",
      "load(dataset='training', path=None)\n",
      "    Import either the training or testing MNIST data set.\n",
      "    It returns a pair with the first element being the collection of\n",
      "    images stacked in columns and the second element being a vector\n",
      "    of corresponding labels from 0 to 9.\n",
      "    \n",
      "    Arguments:\n",
      "        dataset (string, optional): either \"training\" or \"testing\".\n",
      "            (default: \"training\")\n",
      "        path (string, optional): the path pointing to the MNIST dataset\n",
      "            If path=None, it looks succesively for the dataset at:\n",
      "            '/datasets/MNIST' and './MNIST'. (default: None)\n",
      "    \n",
      "    Example:\n",
      "        x, lbl = load(dataset=\"testing\", path=\"/Folder/for/MNIST\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MNISTtools.load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1\n",
      "The shape of xtrain is : (784, 60000)\n",
      "The shape of ltrain is : (60000,)\n",
      "The size of the training dataset is :  60000\n",
      "The feature dimension is :  784\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 1\")\n",
    "\n",
    "xtrain, ltrain = MNISTtools.load(dataset=\"training\", path=\"/datasets/MNIST\")#loaded data\n",
    "\n",
    "print (\"The shape of xtrain is :\",xtrain.shape)\n",
    "print (\"The shape of ltrain is :\",ltrain.shape)\n",
    "print (\"The size of the training dataset is : \",xtrain.shape[1])\n",
    "print (\"The feature dimension is : \",xtrain.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltrain value for that index is : 7\n",
      "They are similar\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 2\")\n",
    "\n",
    "MNISTtools.show(xtrain[:, 42])#displaying the image at that index\n",
    "\n",
    "print (\"ltrain value for that index is :\",ltrain[42])\n",
    "print (\"They are similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 3\n",
      "The minimum of xtrain is : 0\n",
      "The maximum of xtrain is : 255\n",
      "The type of xtrain is : uint8\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 3\")\n",
    "\n",
    "print(\"The minimum of xtrain is :\",np.amin(xtrain))\n",
    "print(\"The maximum of xtrain is :\",np.amax(xtrain))\n",
    "print(\"The type of xtrain is :\",xtrain.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 4\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 4\")\n",
    "\n",
    "xtrain = xtrain.astype(np.float32)#float conversion\n",
    "\n",
    "def normalize_MNIST_images(x):\n",
    "    x = -1.0 + (2*x)/255#mapping [0,255] to [-1,1]\n",
    "    return x\n",
    "\n",
    "\n",
    "xtrain = normalize_MNIST_images(xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 5\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "7\n",
      "We can see that the one-hot code matches.\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 5\")\n",
    "\n",
    "def label2onehot(lbl):#function to convert labels to one-hot codes\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    d[lbl, np.arange(lbl.size)] = 1\n",
    "    return d\n",
    "\n",
    "dtrain = label2onehot(ltrain)\n",
    "print (dtrain[:,42])\n",
    "print (ltrain[42])\n",
    "print (\"We can see that the one-hot code matches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 6\n",
      "Converting back from one-hot to label : 7\n",
      "We can see that it matches ltrain[42]\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 6\")\n",
    "\n",
    "def onehot2label(d):#function to convert one-hot codes back to label\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl\n",
    "\n",
    "ltrain = onehot2label(dtrain)\n",
    "print(\"Converting back from one-hot to label :\",ltrain[42])\n",
    "print (\"We can see that it matches ltrain[42]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 7\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 7\")\n",
    "\n",
    "def softmax(a):#softmax function\n",
    "    M = np.amax(a)\n",
    "    num = den = np.exp(a - M)\n",
    "    y= ((num)/(den.sum(axis=0)))\n",
    "    return y   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 10\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 10\")\n",
    "\n",
    "def softmaxp(a,e):\n",
    "    y =  softmax(a)\n",
    "    element_wise = np.multiply(y,e)\n",
    "    ans = ((element_wise) - (element_wise.sum(axis=0)*y))\n",
    "    return ans\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 11\n",
      "4.915187031417891e-07 should be smaller than 1e-6\n",
      "As we can see, it is smaller than the said value\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 11\")\n",
    "\n",
    "eps         = 1e-6                        # finite difference step\n",
    "a           = np.random.randn(10, 200)    # random inputs\n",
    "e           = np.random.randn(10, 200)    # random directions\n",
    "diff        = softmaxp(a, e)\n",
    "diff_approx = (softmax(a + eps*e) - softmax(a))/(eps)#the approximation formula\n",
    "rel_error   = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "\n",
    "print(rel_error, 'should be smaller than 1e-6')\n",
    "print(\"As we can see, it is smaller than the said value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 12\n",
      "4.2143301280670875e-11 should be smaller than 1e-6\n",
      "As we can see, it is smaller than the said value\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 12\")\n",
    "\n",
    "def relu(a):\n",
    "    return np.maximum(a, 0)\n",
    "\n",
    "def relup(a,e):\n",
    "    y = (a > 0) * e\n",
    "    return y\n",
    "\n",
    "eps         = 1e-6                        # finite difference step\n",
    "a           = np.random.randn(10, 200)    # random inputs\n",
    "e           = np.random.randn(10, 200)    # random directions\n",
    "diff        = relup(a, e)\n",
    "diff_approx = (relu(a + eps*e) - relu(a))/(eps)\n",
    "rel_error   = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "\n",
    "print(rel_error, 'should be smaller than 1e-6')\n",
    "print (\"As we can see, it is smaller than the said value\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 13\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 13\")\n",
    "\n",
    "def init_shallow(Ni, Nh, No):#initiate a network of random values\n",
    "    b1 = np.random.randn(Nh, 1)   / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni)  / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1)   / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh)  / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 14\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 14\")\n",
    "\n",
    "def forwardprop_shallow(x, net):#forward propagate over the network\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "\n",
    "    a1 = W1.dot(x) + b1\n",
    "    #COMPLETE\n",
    "    h1      = relu(a1)\n",
    "    a2      = W2.dot(h1) + b2\n",
    "    y       = softmax(a2)\n",
    "\n",
    "    return y\n",
    "\n",
    "yinit = forwardprop_shallow(xtrain, netinit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 15\n",
      "0.2609550551344122 should be around .26\n",
      "Hence we can see that it satisfies the criteria\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 15\")\n",
    "\n",
    "def eval_loss(y, d):#evaluates average cross-entropy loss\n",
    "    loss_vector = np.multiply(d,np.log(y))\n",
    "    loss = - np.mean(loss_vector)\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "print(eval_loss(yinit, dtrain), 'should be around .26')\n",
    "print(\"Hence we can see that it satisfies the criteria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 16\n",
      "[False False False ... False False False]\n",
      "The percentage of misclassified samples is :  90.09166666666667\n",
      "This is because since a random network is used for classification, there is 1/10 probability that the prediction is correct. Hence we are getting 90% misclassified samples.\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 16\")\n",
    "\n",
    "def eval_perfs(y, lbl):#calculates percentage of misclassified samples\n",
    "    c = np.equal(onehot2label(y), lbl)\n",
    "    print (c)\n",
    "    c = np.sum(c)\n",
    "    c = (c/60000)*100\n",
    "    return 100 - c\n",
    "    \n",
    "print(\"The percentage of misclassified samples is : \",eval_perfs(yinit, ltrain))\n",
    "print(\"This is because since a random network is used for classification, there is 1/10 probability that the prediction is correct. Hence we are getting 90% misclassified samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 17\n",
      "The proof has been attached.\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 17\")\n",
    "\n",
    "def update_shallow(x, d, net, gamma=.05):#updating weights and biases\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "    # \n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    # \n",
    "    d2 = softmaxp(a2, -d/y)\n",
    "    d1 = relup(a1, W2.T.dot(d2))\n",
    "    # \n",
    "    W2 -= gamma*d2.dot(h1.T)\n",
    "    W1 -= gamma*d1.dot(x.T)\n",
    "    b2 -= gamma*d2.sum(axis=1).reshape(No,1)\n",
    "    b1 -= gamma*d1.sum(axis=1).reshape(Nh,1)\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "print (\"The proof has been attached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 18\n",
      "[False  True False ... False False False]\n",
      "The loss is : 0.21196815881309206\n",
      "The training error is : 74.94833333333334\n",
      "[False  True False ... False False False]\n",
      "The loss is : 0.20412524002677554\n",
      "The training error is : 68.97333333333333\n",
      "[False  True False ... False False False]\n",
      "The loss is : 0.19673498489298583\n",
      "The training error is : 63.888333333333335\n",
      "[False  True False ... False False False]\n",
      "The loss is : 0.18968055332466394\n",
      "The training error is : 59.035\n",
      "[False  True False ... False False False]\n",
      "The loss is : 0.1828885552937374\n",
      "The training error is : 54.80833333333333\n",
      "[False  True False ... False False False]\n",
      "The loss is : 0.17630116601378062\n",
      "The training error is : 50.995000000000005\n",
      "[False  True False ... False  True False]\n",
      "The loss is : 0.1698981187684028\n",
      "The training error is : 47.695\n",
      "[False  True False ... False  True False]\n",
      "The loss is : 0.16368890653742793\n",
      "The training error is : 44.708333333333336\n",
      "[False  True False ... False  True False]\n",
      "The loss is : 0.15774713542889324\n",
      "The training error is : 42.14833333333333\n",
      "[False  True False ... False  True False]\n",
      "The loss is : 0.15222607745008035\n",
      "The training error is : 39.83833333333333\n",
      "[False  True False ... False  True False]\n",
      "The loss is : 0.14708445665639763\n",
      "The training error is : 37.85666666666667\n",
      "[False  True False ... False  True False]\n",
      "The loss is : 0.14221798486630013\n",
      "The training error is : 36.068333333333335\n",
      "[False  True False ...  True  True False]\n",
      "The loss is : 0.137599080272382\n",
      "The training error is : 34.516666666666666\n",
      "[False  True  True ...  True  True False]\n",
      "The loss is : 0.13321443165406735\n",
      "The training error is : 32.985\n",
      "[False  True  True ...  True  True False]\n",
      "The loss is : 0.12905205175468595\n",
      "The training error is : 31.723333333333343\n",
      "[False  True  True ...  True  True False]\n",
      "The loss is : 0.12511249981382477\n",
      "The training error is : 30.40166666666667\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.12138561013641953\n",
      "The training error is : 29.393333333333345\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.11786070041988013\n",
      "The training error is : 28.233333333333334\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.11452527771707793\n",
      "The training error is : 27.45833333333333\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.11136571917549799\n",
      "The training error is : 26.49333333333334\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.1083721365630602\n",
      "The training error is : 25.86500000000001\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.10553875779518819\n",
      "The training error is : 24.951666666666668\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.1028609102027906\n",
      "The training error is : 24.58333333333333\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.10034004100489889\n",
      "The training error is : 23.568333333333342\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.09797809896271195\n",
      "The training error is : 23.51166666666667\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.09577722378254441\n",
      "The training error is : 22.48166666666667\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.09376015270453793\n",
      "The training error is : 22.763333333333335\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.09191206660160393\n",
      "The training error is : 21.678333333333327\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.09038252307447846\n",
      "The training error is : 22.58333333333333\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.08897658026384024\n",
      "The training error is : 21.536666666666676\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.08834451422507462\n",
      "The training error is : 23.578333333333333\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.08727754477710613\n",
      "The training error is : 22.14\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.0881235375443347\n",
      "The training error is : 25.346666666666664\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.08619749083325977\n",
      "The training error is : 22.760000000000005\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.08792034438280122\n",
      "The training error is : 26.27333333333334\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.08407044410629883\n",
      "The training error is : 22.09166666666667\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.08499692917695983\n",
      "The training error is : 25.620000000000005\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.0809605277762379\n",
      "The training error is : 21.12166666666667\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.08117727961826345\n",
      "The training error is : 24.224999999999994\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.07768765605532381\n",
      "The training error is : 20.155\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.07751947114858851\n",
      "The training error is : 22.570000000000007\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.07476588812290835\n",
      "The training error is : 19.398333333333326\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.07442723866737068\n",
      "The training error is : 21.126666666666665\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.07228789022840565\n",
      "The training error is : 18.763333333333335\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.07188489256842213\n",
      "The training error is : 20.03333333333333\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.07018449846483125\n",
      "The training error is : 18.328333333333333\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.06976971344317204\n",
      "The training error is : 19.176666666666662\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.06837200412655316\n",
      "The training error is : 17.968333333333334\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.06797308376977461\n",
      "The training error is : 18.46166666666666\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.0667906188639469\n",
      "The training error is : 17.703333333333333\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.0664171332503026\n",
      "The training error is : 17.938333333333333\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.06538880410541455\n",
      "The training error is : 17.5\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.06503965124916795\n",
      "The training error is : 17.540000000000006\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.06412573771169654\n",
      "The training error is : 17.281666666666666\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.06379091082659624\n",
      "The training error is : 17.183333333333323\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.06297039700115127\n",
      "The training error is : 17.096666666666664\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.06264554908482686\n",
      "The training error is : 16.88166666666666\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.061896026714692465\n",
      "The training error is : 16.924999999999997\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.061572085604326206\n",
      "The training error is : 16.583333333333343\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.06087845909024812\n",
      "The training error is : 16.73166666666667\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.060547732585492293\n",
      "The training error is : 16.308333333333337\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.05990122775419289\n",
      "The training error is : 16.52333333333334\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.05956548294785646\n",
      "The training error is : 16.054999999999993\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.0589599579689484\n",
      "The training error is : 16.313333333333333\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.058619772749316096\n",
      "The training error is : 15.766666666666666\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.05805229455743454\n",
      "The training error is : 16.09166666666667\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.05771010025382859\n",
      "The training error is : 15.546666666666667\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.05717775199622686\n",
      "The training error is : 15.86333333333333\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.05683682353951389\n",
      "The training error is : 15.310000000000002\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.056340278430066455\n",
      "The training error is : 15.625\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.056005351529891195\n",
      "The training error is : 15.060000000000002\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.05554121102580211\n",
      "The training error is : 15.403333333333336\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.05521584931076593\n",
      "The training error is : 14.873333333333335\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.054783165461671564\n",
      "The training error is : 15.169999999999987\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.05446872200002141\n",
      "The training error is : 14.701666666666668\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.05406306848831048\n",
      "The training error is : 14.976666666666674\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.05375937322013152\n",
      "The training error is : 14.510000000000005\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.05338138857862652\n",
      "The training error is : 14.791666666666671\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.05308883054224841\n",
      "The training error is : 14.338333333333324\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.052734346009654834\n",
      "The training error is : 14.614999999999995\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.0524528054206503\n",
      "The training error is : 14.168333333333337\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.05212056740002614\n",
      "The training error is : 14.450000000000003\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.05184999332269487\n",
      "The training error is : 14.034999999999997\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.05154133757765732\n",
      "The training error is : 14.258333333333326\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.051281415862521634\n",
      "The training error is : 13.909999999999997\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.05099244090552137\n",
      "The training error is : 14.12166666666667\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.05074286932371008\n",
      "The training error is : 13.77000000000001\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.05046884144247903\n",
      "The training error is : 13.985\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.0502296477173934\n",
      "The training error is : 13.674999999999997\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.04997047046291217\n",
      "The training error is : 13.838333333333324\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.04974005108415713\n",
      "The training error is : 13.560000000000002\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.04949497121686797\n",
      "The training error is : 13.671666666666667\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.04927326109189782\n",
      "The training error is : 13.456666666666663\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.04904097189509447\n",
      "The training error is : 13.59833333333333\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.04882795128776544\n",
      "The training error is : 13.333333333333329\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.04860665962539456\n",
      "The training error is : 13.521666666666661\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.04840240419511806\n",
      "The training error is : 13.239999999999995\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.04819137105557332\n",
      "The training error is : 13.40166666666667\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.04799487902771333\n",
      "The training error is : 13.163333333333341\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.0477940030431145\n",
      "The training error is : 13.303333333333327\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.04760496071251265\n",
      "The training error is : 13.053333333333327\n",
      "[False  True  True ...  True  True  True]\n",
      "The loss is : 0.04741332033484262\n",
      "The training error is : 13.189999999999998\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 18\")\n",
    "\n",
    "def backprop_shallow(x, d, net, T, gamma = 0.05):#backpropagation function\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        net = update_shallow(x,d,net,gamma)\n",
    "        y = forwardprop_shallow(x,net)\n",
    "        loss = eval_loss(y,d)\n",
    "        training_error = eval_perfs(y,lbl)\n",
    "        print (\"The loss is :\",loss)\n",
    "        print (\"The training error is :\",training_error)\n",
    "        \n",
    "    return net\n",
    "\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 2)\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 18 Contd\n",
      "As we can see, the network has achieved training error of 13.19% after 100 iterations\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 18 Contd\")\n",
    "print (\"As we can see, the network has achieved training error of 13.19% after 100 iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 19\n",
      "The shape of xtest is: (784, 10000)\n",
      "The shape of ltest is: (10000,)\n",
      "The size of the testing dataset is : 10000\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is :  0.09060106537555575\n",
      "The testing error is:  87.39\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 19\")\n",
    "\n",
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "xtest = normalize_MNIST_images(xtest)\n",
    "dtest = label2onehot(ltest)\n",
    "\n",
    "print(\"The shape of xtest is:\",xtest.shape)\n",
    "print(\"The shape of ltest is:\",ltest.shape)\n",
    "print(\"The size of the testing dataset is : 10000\")\n",
    "\n",
    "y = forwardprop_shallow(xtest,nettrain)#testing on test data\n",
    "loss = eval_loss(y,dtest)\n",
    "testing_error = eval_perfs(y,ltest)\n",
    "\n",
    "print(\"The loss is : \",loss)\n",
    "print (\"The testing error is: \",training_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 20\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 20\")\n",
    "\n",
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):#minibatch gradient descent method\n",
    "    N = x.shape[1]\n",
    "    lbl = onehot2label(d)\n",
    "    NB =int((N+B-1)/B)\n",
    "    \n",
    "    for t in range(T):\n",
    "        for l in range(NB):\n",
    "            idx = np.arange(B*l, min(B*(l+1), N))\n",
    "            net = update_shallow(x[:,idx],d[:,idx],net,gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        loss = eval_loss(y,d)\n",
    "        training_error = eval_perfs(y,lbl)\n",
    "        print(loss)\n",
    "        print(training_error)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 21\n",
      "[ True  True  True ...  True  True  True]\n",
      "0.00638118087250069\n",
      "2.001666666666665\n",
      "[ True  True  True ...  True  True  True]\n",
      "0.00620677356344962\n",
      "1.9533333333333331\n",
      "[ True  True  True ...  True  True  True]\n",
      "0.0059672864272528525\n",
      "1.88333333333334\n",
      "[ True  True  True ...  True  True  True]\n",
      "0.005843186752013941\n",
      "1.836666666666659\n",
      "[ True  True  True ...  True  True  True]\n",
      "0.005717637543621029\n",
      "1.806666666666672\n",
      "[ True  True  True ...  True  True  True]\n",
      "The loss is : 0.027083609668869593\n",
      "The testing error is : 84.75166666666667\n"
     ]
    }
   ],
   "source": [
    "print (\"Answer 21\")\n",
    "\n",
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100)\n",
    "y = forwardprop_shallow(xtest,netminibatch)\n",
    "loss = eval_loss(y,dtest)\n",
    "testing_error = eval_perfs(y,ltest)\n",
    "\n",
    "print(\"The loss is :\",loss)\n",
    "print (\"The testing error is :\", testing_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
